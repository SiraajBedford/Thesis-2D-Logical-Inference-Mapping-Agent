\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A general architecture for a learning agent interfacing with an environment with sensors and actuators.\relax }}{5}{figure.caption.13}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Knowledge-based Agent that uses Logical Inference\relax }}{12}{figure.caption.21}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of proof by resolution using example propositions $p,q,r,s$ and $t$\relax }}{15}{figure.caption.22}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Example Simulation of Agent in an Environment\relax }}{19}{figure.caption.23}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Reference for propositional symbols used\relax }}{20}{figure.caption.24}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Knowledge-based agent design.\relax }}{22}{figure.caption.26}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Agent in Environment 1.\relax }}{25}{figure.caption.27}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces A simulation of 200 iteration for the agent using the DPLL algorithm for Environment 1.\relax }}{26}{figure.caption.28}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Agent's learnt state definitions for Environment 1.\relax }}{27}{figure.caption.30}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Agent's learnt state transitions for Environment 1.\relax }}{27}{figure.caption.31}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Agent's learnt structure of Environment 1 and actual structure of Environment 1 represented as directed graphs.\relax }}{28}{figure.caption.32}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Agent state predictions for Environment 1.\relax }}{28}{figure.caption.33}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Agent in Environment 2.\relax }}{29}{figure.caption.34}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces A simulation of 200 step for the agent using the DPLL algorithm for Environment 2.\relax }}{29}{figure.caption.35}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Agent's learnt state definitions for Environment 2.\relax }}{30}{figure.caption.37}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Agent's learnt state transitions for Environment 2.\relax }}{31}{figure.caption.38}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Agent's learnt structure of Environment 2 expressed as a directed graph.\relax }}{31}{figure.caption.39}%
\contentsline {figure}{\numberline {5.12}{\ignorespaces Actual structure of Environment 2 expressed as a directed graph.\relax }}{32}{figure.caption.40}%
\contentsline {figure}{\numberline {5.13}{\ignorespaces Agent state predictions for Environment 2.\relax }}{32}{figure.caption.41}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
