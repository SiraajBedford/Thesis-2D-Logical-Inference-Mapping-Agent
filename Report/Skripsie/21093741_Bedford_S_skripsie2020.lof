\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A general architecture for a learning agent interfacing with an environment with sensors and actuators.\relax }}{5}{figure.caption.13}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Knowledge-based Agent that uses Logical Inference\relax }}{13}{figure.caption.21}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of proof by resolution using example propositions $p,q,r,s$ and $t$\relax }}{16}{figure.caption.22}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Firs-Order Logic Inference\relax }}{20}{figure.caption.23}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces High level mapping agent and environment design diagram\relax }}{21}{figure.caption.24}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Inference Engine\relax }}{22}{figure.caption.25}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Example Simulation of Agent in an Environment\relax }}{24}{figure.caption.26}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Agent Execution Flow\relax }}{25}{figure.caption.27}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Reference for propositional symbols used\relax }}{26}{figure.caption.28}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Possible state transitions in terms of a directed graph\relax }}{28}{figure.caption.30}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Agent in Environment 1.\relax }}{31}{figure.caption.31}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces A simulation of 200 iteration for the agent using the DPLL algorithm for Environment 1.\relax }}{32}{figure.caption.32}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Agent's learnt state definitions for Environment 1.\relax }}{33}{figure.caption.34}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Agent's learnt state transitions for Environment 1.\relax }}{33}{figure.caption.35}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Agent's learnt structure of Environment 1 and actual structure of Environment 1 represented as directed graphs.\relax }}{34}{figure.caption.36}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Agent state predictions for Environment 1.\relax }}{34}{figure.caption.37}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Agent in Environment 2.\relax }}{35}{figure.caption.38}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces A simulation of 200 step for the agent using the DPLL algorithm for Environment 2.\relax }}{35}{figure.caption.39}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Agent's learnt state definitions for Environment 2.\relax }}{36}{figure.caption.41}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Agent's learnt state transitions for Environment 2.\relax }}{37}{figure.caption.42}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Agent's learnt structure of Environment 2 expressed as a directed graph.\relax }}{37}{figure.caption.43}%
\contentsline {figure}{\numberline {5.12}{\ignorespaces Actual structure of Environment 2 expressed as a directed graph.\relax }}{38}{figure.caption.44}%
\contentsline {figure}{\numberline {5.13}{\ignorespaces Agent state predictions for Environment 2.\relax }}{38}{figure.caption.45}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
