\chapter{Literature} 
\label{chapter:Literature}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ELO 1, 6, 8, 9
% Planning for Literature

% Chapter 2: Literature

% 2.1  Related Work - Their opbjective (Similar to yours), their method, their results, short comings wrt YOUR objective. Highlight their downfall/problems/remianing challenges

% 2.2 Literature - What information did you need to learn, to be able to solve the problem and do the design. This must be evident to the examiner.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Related Work}
%
%\subsection{Paper 1}
%\subsubsection{Objectives}
%\subsubsection{Methods}
%\subsubsection{Their results}
%\subsubsection{Their shortcomings/remaining challenges}

%\textit{learn}
%\textsc{learn}
%\textsf{learn}
%\textsl{learn}
%\texttt{learn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Agents and Environments}
\label{sec:agents_and_environments}

\begin{figure}[H]
    \centering
    \includegraphics[scale=.5]{Figures/General_learning_agent.png}
    \caption{A general learning agent as in \citep{russell2016artificial}}
    \label{fig:agent_environment}
\end{figure}

An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actions \citep{russell2016artificial}. Agents in environments have a main purpose to \textit{learn} things about the environment or to formulate plans that have certain objectives in the context of the environment. This can, to a large extent be influenced by the nature of the environment and internal processes of the agent. Figure \ref{fig:agent_environment} shows the relationship between the agent and the environment. 

The general learning agent comprises of four main conceptual components: a performance element, a learning element, a critic and a problem generator. The most crucial difference is between the \textbf{learning element} which makes improvements and the \textbf{performance element} which is responsible for responsible for selecting external actions (that may aim to make the agent learn its function in the environment or just to build a representation of the environment). The performance element takes in sensor information and decides on which action to take by consulting the learning element. The learning elemment uses informaton from the criric to see how good the agent the agent is doiung in reference to a performance measure so that the performabce element should be modified or not because. 


% Talk about nauture of environments



% Say the connections between subcomponents are related by logic (to lead into the next sexction)



%%%%%%%%%%%%%%%%%%%%%%%%%























%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Propositional Logic}

%We now present a simple but powerful logic called propositional logic. We cover the syntax
%of propositional logic and its semantics—the way in which the truth of sentences is deter-
%mined. Then we look at entailment—the relation between a sentence and another sentence
%that follows from it—and see how this leads to a simple algorithm for logical inference. Ev-
%erything takes place, of course, in the wumpus world.

There are two common types of logic representations used for agents that use knowledge base learning, namely propositional logic and first-order logic. Propositional logic  represents statements that can either be true or false representing knowledge in a logical and mathematical form \cite{Logic}. First-order logic builds on propositional logic as it has the ability to articulate or quantify variables. Quantifying the meaning of variables is not of any use in this project and thus PL is used.
%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Syntax}



% Add truth tables for each logical connective
%The syntax of propositional logic defines the allowable sentences. The atomic sentences
%consist of a single proposition symbol. Each such symbol stands for a proposition that can
%be true or false. We use symbols that start with an uppercase letter and may contain other
%letters or subscripts, for example: P , Q, R, W 1,3 and North. The names are arbitrary but
%are often chosen to have some mnemonic value—we use W 1,3 to stand for the proposition
%that the wumpus is in [1,3]. (Remember that symbols such as W 1,3 are atomic, i.e., W , 1,
%and 3 are not meaningful parts of the symbol.) There are two proposition symbols with fixed
%meanings: True is the always-true proposition and False is the always-false proposition.
%Complex sentences are constructed from simpler sentences, using parentheses and logical
%connectives. There are five connectives in common use:

Sentences written in propositional logic can be atomic, i.e. consist of one logical variable or a complex sentences consisting of atomic variables joined by logical connectives shown in Table \ref{table: BNF_Syntax}.

\begin{table}[H]
  \centering
  \begin{tabular}{lc}
    \toprule

    \textbf{Propositional Logic}  \hspace{1cm}   & \textbf{Expression}  \\
    \toprule
    
    $Sentence$ & $AtomicSentence$ | $ComplexSentence$ \\ \midrule
    
    $AtomicSentence$ & \textit{True} | \textit{False} | \textit{$P$} | \textit{$Q$} | \textit{$R$} | ... 
    \\  \midrule
    
    $ComplexSentence$ & $(Sentence)$ $|$ $[Sentence]$  \\
     \tabitem Negation & $\neg$ $Sentence$  \\
     \tabitem Conjunction & $Sentence$ $\wedge$ $Sentence$  \\
     \tabitem Disjunction & $Sentence$ $\vee$ $Sentence$  \\
     \tabitem Implication & $Sentence$ $\Rightarrow$ $Sentence$  \\
     \tabitem Biconditional & $Sentence$ $\Leftrightarrow$ $Sentence$  \\
   	 \midrule
   
	Operator Precedence & $\neg,\wedge,\vee,\Rightarrow,\Leftrightarrow$   \\
   
    \bottomrule
  \end{tabular}
  \caption{PL BNF (Backus-Naur Form) grammar of sentences in propositional logic along with operator precedences, form highest to lowest as in \citep{russell2016artificial}}
  \label{table: BNF_Syntax}
\end{table}







\begin{table}[H]
  \centering
  \begin{tabular}{lc}
    \toprule

    \textbf{Propositional Logic}  \hspace{1cm}   & \textbf{Expression}  \\
    \toprule
    
    $CNFSentence$ & $Clause_1$ $\wedge$ $\cdots$  $\wedge$ $Clause_n$ \\ \midrule
    $Clause$ & $Literal_1$  $\vee$ $\cdots$  $\vee$  $Literal_m$   \\ \midrule  
    $Literal$ & $Symbol$ | $\neg Symbol$    \\ \midrule
    $Symbol$ & \textit{$P$} | \textit{$Q$} | \textit{$R$} | ...  \\  \midrule

   
	 $HornClauseForm$ & $DefiniteClauseForm$ | $GoalClauseForm$    \\ 
	 $DefiniteClauseForm$ & ($Symbol_1$ $\wedge$ $\cdots$  $\wedge$ $Symbol_l$) $\Rightarrow$ $Symbol$   \\ 
	 $GoalClauseForm$ & ($Symbol_1$ $\wedge$ $\cdots$  $\wedge$ $Symbol_l$) $\Rightarrow$ $False$    \\   
   
    \bottomrule
  \end{tabular}
  \caption{PL CNF (Conjunctive Normal Form) grammar of sentences in propositional logic along with operator precedences, form highest to lowest as in \citep{russell2016artificial}}
  \label{table: BNF_Syntax}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%












%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantics}

%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=.6]{Figures/propositional_logic_semantics_1.png}
%    \caption{Semantcis 1}
%    \label{fig:S1}
%\end{figure}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=.6]{Figures/propositional_logic_semantics_2.png}
%    \caption{Semantcis 2}
%    \label{fig:S2}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%










\section{Logical Inference}
Goal of inference is to prove entailment of a sentence in a knowledge base. Computational complexity is $\mathcal{O}(2^n)$

\begin{figure}[H]
    \centering
    \includegraphics[scale=2]{Figures/Inference_Diagram.png}
    \caption{Logical inference in context}
    \label{fig:entailment}
\end{figure}




\subsection{Model Checking}
Enumeration of models.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{Figures/model_checking.png}
    \caption{Logical inference in context}
    \label{fig:entailment}
\end{figure}

\subsection{Theorem proving}

In many practical cases finding a proof can be more efficient because the proof can ignore irrelevant propositions, no
matter how many of them there are \citep{russell2016artificial}.



\subsubsection{Proof by Resolution}



\begin{equation}
\dfrac{ l_1 \vee \cdots \vee l_k,\hspace{1cm}  m_1 \vee \cdots \vee m_n}{    l_1 \vee \cdots \vee l_{i-1} \vee l_{i+1} \vee \cdots \vee l_k  \vee m_1 \vee \cdots \vee m_{j-1} \vee m_{j+1} \vee \cdots \vee m_n }
\label{eq:resolution}
\end{equation}

\subsubsection{Proof by Unsatisfiability}







%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Graph Theory}
%\subsection{Directed Graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%









