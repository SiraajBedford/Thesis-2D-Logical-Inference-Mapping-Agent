%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ELO 2,3,5,6
% Planning for Detailed Design

% Chapter 4: Detailed Design

% System Diagram with functional blocks 

% 4.1 Detailed design of component 1
% I chose to use a Mongo database instead of {x,y,z} because of ...
% List technical details

% 4.2 Detailed design of component 2

% 4.3 Detailed design of component 3

% 4.4 Detailed design of component 4v
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Low-Level System Design} 
\label{LL_design}


\section{Agent-Environment Interface}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.73]{Figures/Goal2.png}
    \caption{Example Simulation of Agent in an Environment} 
    \label{fig:agent_in_env_low_level}
\end{figure}


The environment is represented as a 2D grid world as shown in Figure \ref{fig:agent_in_env_low_level} and the agent is represented as one block (the white block) in the environment. The environment consists of open blocks and closed blocks. The agent is allowed to roam on open blocks (shown as black blocks) and not allowed to move onto closed blocks (shown as grey blocks). The different simulation environments used in this project bound the agent to ensure that the agent's next position is known and not random, which would not be the case if rules were implemented in an unbounded environment that made the agent's next position unknown if the agent moved out of bounds.
The environment is partially observable since the agent sensor only measures the surrounding blocks of the agent and is further discussed in Subsection \ref{subsubsec: sensor}. The environment is deterministic since the agent can move from one state to another by means of an action or series of actions. The environment is further defined to be a single-agent environment as only one agent occupies the environment in this project. The environment is static because the environment does not change as the agent moves through it, i.e., the blocks in the environment are fixed. Finally, the environment is discrete since there are a finite number of actions that the agent can do in the environment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Knowledge Based Agent}

The type of agent we use in this project is a KB Agent. This agent allows us to implement the inference algorithms discussed in Sections \ref{subsubsec: Inference_model_checking} and \ref{subsubsec: Inference_PL_theorem_proving} to query the information stored in a KB over time. The execution flow of the agent to create a representation of the environment it occupies is diagrammatically shown in Figure \ref{fig:agent_in_env_low_level}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.62]{Figures/low_level_state_gen.png}
    \caption{Agent Execution Flow} 
    \label{fig:agent_in_env_low_level}
\end{figure}

The execution starts by using the agent sensor to take measurements from the environment and stores those measurements as propositional symbols with time indices in a KB consisting of action and percepts symbols as defined in Section \ref{subsec: sensor_prop_symbols}. The reason we use time indices is so that we can differentiate between sensor measurements from one time step to the next. Next, the agent needs to know which actions and percepts it experienced, as such the agent is asked a series of questions about the measurements it acquired. Through the process of logical inference, the agent represents the true actions and percepts it measures as a positive literals (described in Section \ref{subsubsec:CNF}) and false percepts and actions as negative literals. The literals are then logically combined to form and generate state definitions as logical sentences with time indices. These state definitions as well as actions performed to move from one state to the next are stored as logical sentences in a list. Thus, every time step, we can form a state transition. We can use the state transitions to form a representation of the environment by evaluating the moves in the state transitions to build a directed graph. The directed graph removes the time indices of states and actions to generalise the environment representation to be independent of the movements the agent took to acquire the sensor measurements.


\subsection{Sensor Input and Reference Propositional Symbols}
\label{subsec: sensor_prop_symbols}

The sensor measurements the agent takes consists of whether the the immediate blocks surrounding the agent are open or close as well as which actions the agent performs from one time step to the next. The propositional symbols for the sensing if the surrounding blocks are open or closed are shown in Figure \ref{subfig:agent_in_env_low_level_percepts}, with each surrounding block corresponding to the propositional symbols for that block. The meaning of these symbols are explained in Table \ref{table:symbol_description}. The propositional symbols for actions are shown in  Figure \ref{subfig:agent_in_env_low_level_action}. Each coloured arrow in the figure corresponds to the direction the agent can move and which propositional symbol corresponds to that action. The meaning of these percepts are defined in Table \ref{table:symbol_description}. When the agent tries to execute an action and occupies the same block at the next time step, it means that the agent tried to move onto an closed block and it bumped against the wall, the propositional symbol for this action is $MB_{t}$ in Table \ref{table:symbol_description}.

\begin{figure}[H]
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.6]{Figures/Low_level_design_percepts.png}
    \caption{Reference for agent percepts proposition symbols at time \textit{t}} 
    \label{subfig:agent_in_env_low_level_percepts}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.37]{Figures/Low_level_design_actions.png}
    \caption{Reference for agent action proposition symbols at time \textit{t}} 
    \label{subfig:agent_in_env_low_level_action}
\end{subfigure}
\caption{Reference for propositional symbols used}
\label{fig:test}
\end{figure}



\begin{table}[H]
  \begin{center}
    
    \begin{tabular}{c|l}
    
      \textbf{Symbol} & \textbf{Description} \\ 
      \hline
      $L_t$ & The block to the left of the agent is open at time \textit{t}. \\ 
      $R_t$ & The block to the right of the agent is open at time \textit{t}. \\  
      $U_t$ & The block to the top of the agent is open at time \textit{t}. \\   
      ${UL}_t$ & The block to the upper left of the agent is open at time \textit{t}. \\  
      ${UR}_t$ & The block to the upper right of the agent open at time \textit{t}. \\  
      $B_t$ & The block to the bottom of the agent is open at time \textit{t}. \\  
      ${BL}_t$ & The block to the bottom left of the agent is open at time \textit{t}. \\  
      ${BR}_t$ & The block to the bottom right of the agent is open at time \textit{t}. \\    
      ${ML}_t$ & The agent moved left from the previous state to occupy a certain block at time \textit{t}.\\ 
      ${MR}_t$ & The agent moved right from the previous state to occupy a certain block at time \textit{t}.\\ 
      ${MU}_t$ & The agent moved up from the previous state to occupy a certain block at time \textit{t}.\\  
      ${MD}_t$ & The agent moved down from the previous state to occupy a certain block at time \textit{t}.\\  
      ${MB}_t$ & The agent bumped against a closed block from the previous state at time \textit{t}.
    \end{tabular}
  \end{center}
\caption{Meanings of propositional symbols used for logical sentence design.}
\end{table}
\label{table:symbol_description}



\subsection{State Sentence Formulation and Generation}

The partial states of the environment the agent can experience can be expressed as logical sentence of ANDs of the surrounding blocks the agent perceives in the state as well as the action(s) the agent performed or did not perform to get to that state. The percepts sentence components can be summarised as a column vector as seen in Equation \ref{eq:percept_vector} and the action sentence can also be summarised this way as shown in Equation \ref{eq:action_vector}. The state sentence $\mathcal{S}_t$ can then be represented as the implication of logical ANDs of the percept and action sentences at time \textit{t} as seen in Equation \ref{eq:state_vector}.

\begin{multicols}{2}
  \begin{equation}
    {P_t}=
    \begin{bmatrix}
    		(L_t \vee \neg L_t)\\
    		(R_t \vee \neg R_t)\\
    		(U_t \vee \neg U_t)\\
    		(UL_t \vee \neg UL_t)\\
    		(UR_t \vee \neg UR_t)\\
    		(B_t \vee \neg B_t)\\
    		(BL_t \vee \neg BL_t)\\
    		(BR_t \vee \neg BR_t)\\
    \end{bmatrix}
    \label{eq:percept_vector}
  \end{equation}\break
  \begin{equation}
    {A_t}=
    \begin{bmatrix}
    		(ML_t \vee \neg ML_t)\\
    		(MR_t \vee \neg MR_t)\\
    		(MU_t \vee \neg MU_t)\\
    		(MD_t \vee \neg MD_t)\\
    		(MB_t \vee \neg MB_t)\\
    \end{bmatrix}
     \label{eq:action_vector}
  \end{equation}
\end{multicols}



\begin{equation}
      \mathcal{S}_t \Rightarrow AND ({P_t}^T) \wedge AND ({A_t}^T)
 \label{eq:state_vector}
\end{equation}


State definitions generlised by Equation \ref{eq:state_vector} are generated through logical inference as the agent moves through the environment. This logical inference process is shown in Figure \ref{fig:agent_in_env_low_level}. The inference procedure uses the KB consisting of actions and percepts. This KB can be cleared after inference is carried out from time step to the next to allow for a constant KB size. Therefore, the inference process will be executed quickly to build state sentences. While the agent moves through the environment, it builds a list of logical state definitions and actions that form a state transition from one time step to the next. This list will allow us to build a directed graph that forms a representation of the environment from state transitions. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Environment Representation Creation}

The structural nature of a directed graphs, i.e., that they contain vertices and edges were suitable to represent the environment. States could be represented as vertices and directed edges between vertices could represent action(s) to transition from one state to another. Each vertex has a label and a description, the state name is stored as the label and the a state description is stored in vertex memory for retrieval after the environment representation is created. State definitions are expanded on as the agent the agent moves through the environment (and thereby the directed graph). This is possible since the percepts for the action sentence might be the same for that position in the environment, but the action sentence forming part of the state definition can be different, put simply, the agent can reach the same state in different ways and therefore the state definition for that state can be expanded over time. 
There are two possible ways for the agent to interpret a state transition. Either the agent can move to another block, or it can bump against a closed block and stay in the same position and therefore the same state:


\begin{enumerate}
	\item The agent bumps against a closed block. An edge looping to the same vertex can be created since this will signify that the agent did not move when trying to execute an action.
	\item The agent did not bump into a closed block and transitioned to another state by means of moving left, right, up or down. An edge can be created from one vertex to another representing the transition.
\end{enumerate}

These two state transition generalisations are conceptualised in Figure \ref{fig:state_diagram_concept} and further shown in Algorithm \ref{algorithm:Directed_Graph_building_algorithm}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/Low_level_design_directed_graph.png}
    \caption{Possible state transitions in terms of a directed graph} 
    \label{fig:state_diagram_concept}
\end{figure}


\vspace{0.5cm}
\begin{algorithm}[H]
\label{algorithm:Directed_Graph_building_algorithm}
\caption{\textsc{Directed Graph Building Algorithm}}
\SetAlgoLined
\DontPrintSemicolon
\KwIn{$S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$, A State Transition,  \newline $\mathcal{B} = \{\}$, An empty list to store move history}
\KwOut{$G=(V,E)$, A Directed Graph Representation of the Environment}
\textbf{Begin} \\
\Indp{
	$\mathcal{A}$ = \textsc{Extract Moves} \{$S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$\}\\
	\lIf{$\mathcal{A}$ contains $MB_t$ and ($ML_t$ or $MR_t$ or $MU_t$ or $MD_t$) }{\\
		\hspace{1cm} $S_t = S_{t+1}$ 		\\
		\hspace{1cm} Create vertex with label $S_{t}$ if not already created	\\
		\hspace{1cm} Create an edge looping from $S_t$ to $S_{t+1}$ (Same vertex) with action label\\
		\hspace{1cm} Store state definition in vertex memory\\
	}	
	
	\lIf{$\mathcal{A}$ contains only ($ML_t$ or $MR_t$ or $MU_t$ or $MD_t$)}{\\
		\hspace{1cm} Create vertex with label $S_{t}$ if not already created	\\		
		\hspace{1cm} Create vertex with label $S_{t+1}$	\\	
		\hspace{1cm} Create an edge from $S_t$ to $S_{t+1}$ with action label\\
		\hspace{1cm} Store state definition in vertex $S_{t+1}$ memory\\
		\hspace{1cm} $\mathcal{B} \leftarrow \{\mathcal{B}, \mathcal{A} \}$\\
	}	
	
	\lIf{$\mathcal{B}$ has a move sequence showing the agent is in an discovered state}{\\
		\hspace{1cm} Backtrack to the original vertex\\
		\hspace{1cm} Create an edge from $S_t$ to the backtracked vertex with action label\\
		\hspace{1cm} Store state definition in backtracked vertex memory\\
		\vspace{0.1cm}
		return Updated $G(V, E)$
	}	
	
	}
\Indm 
\textbf{End}   \\
\end{algorithm}
\vspace{0.5cm}

Algorithm \ref{algorithm:Directed_Graph_building_algorithm} is used by the agent to build the directed graph from the list of state transitions. The input to the algorithm per time step is one state transition from state transition list, $S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$. The function also keeps a constant list variable for the history of the actions the agent made, excluding actions when the agent did not move. This list is arbitrarily named $\mathcal{B}$. 

As the agent moves through the environment, it builds the directed graph $G$. There are 3 cases the agent considers which are shown as if-statements in lines 3, 9, and 16 and are explained in subsequent lines according to the meaning of the percepts in Table \ref{table:symbol_description}. 

Once the graph has been fully built by the agent using Algorithm \ref{algorithm:Directed_Graph_building_algorithm} (the number of vertices must equal the number of blocks the agent can freely roam on since each of these blocks represent a different state the agent can be in), each vertex in $G$ will contain a state description that can fully define a particular state of the agent in the environment. As stated before, each vertex is joined to another by means of edges representing actions for state state transitions. The agent must uses the state definitions to perform inference to identify where itis in the environment.

sdfsdfdsf




















%\citep{Discrete_Maths}
\newpage
