%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ELO 3, 6
% Planning for System Design

% Chapter 3: Agent Design

% System Diagram with functional blocks 

% 3.1.1 Knowledge Base
% To store knowledge I could have used option 1,2 or 3, (List options at disposal) No indepth level. It is important you introduced the examiner to the concepts in Chapter 2 to see the requirements for such a functional block. 

% 3.1.2 Functional Block 2 ...

% *Put intefaces between functional block, e.g. I2C, SPI for an electrical project. 

% 3.2 Metrics
% I want to measure whether I meet the objectives 1,2,3. This will say how/what I will measure. What can actualy be masued and what cannot be measured. Each metrics must prove that you can reach the requirement.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Agent Design} 
\label{chapter:Agent_Design}

\begin{figure}[H]
    \centering
    \includegraphics[scale=.35]{Figures/main_architecture.PNG}
    \caption{High level mapping agent and environment design diagram} 
    \label{fig:agent_and_environment_design}
\end{figure}

%Fill the functional blocks with more "color"
%Also put interfaces between blocks using arrows.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{System Architecture}
%Show agent in environment with robot in environment, actuators, remember agent controls the robot and the robot is in the physical environment.
%Show all possible choices you could have made for reach of the functional blocks.

This chapter goes over in detail how the agent-environment interaction is to be designed on a high level. Figure \ref{fig:agent_and_environment_design} shows the high-level design of the agent with respect to the environment.
It is obvious that the environment is minimalistic in defined structure since it is the purpose of the agent to map the environment without knowing anything about the environment. The only narrative the agent can build of the environment is from the measurements it \textit{perceives} with its sensors. The agent is based on logic and thus it needs a logic engine, which underpins most of the operation of the agent due to the agent needing a \textit{knowledge representation language} mentioned in section \ref{sec:kb_agents} to convey knowledge.
It must also be conveyed that the agent in Figure \ref{fig:agent_and_environment_design} does not necessarily mirror the general learning agent in section \ref{sec:agents_and_environments}. This is because the project captured in this report builds on the concepts describing agents based on knowledge bases in section \ref{sec:kb_agents}. The purpose of section \ref{sec:agents_and_environments} was to introduce the reader to the concept of \textit{an agent in an environment}. The mapping agent 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% reference objectives: Logical inference, KB memory management, learning state definitions, representative knowledge graphing 

\section{Mapping Agent}		

\subsection{Logic Engine}
The logic engine is what allows the agent's internal processes to communicate. Thus, it had to have the the ability to handle two processes quickly and efficiently, namely \textit{sentence management} as well as the ability to \textit{perform logic operations and equivalences} on those sentences. There were two options to implement a logic engine: either write one from scratch, or use an existing logic engine with known performance, i.e., if it is suitable for the context of this project. Since the aim of this project is not to build a logic engine, source code written with an MIT license was used. This code base was based on direct examples written in \citep{russell2016artificial} and as such, was highly suitable and implementable.

\subsection{Inference Engine}
The inferential component of the system is the core component we need to design as it will allow the agent to come to conclusions about the environment as the agent moves through the environment. There are three algorithms that were considered when formulating an inference plan, namely, model checking, proof by resolution and the use of the DPLL algorithm to prove unsatisfiability. All three of these inference methods are detailed in section \ref{sec:logical_inference}. All three algorithms will be experimented with as described in section \ref{sec:experimental_method}

\begin{figure}[H]
    \centering
    \includegraphics[scale=.6]{Figures/Inference_HL.PNG}
    \caption{Inference Engine} 
    \label{fig:inference_engine}
\end{figure}

\subsection{Knowledge Base}
%Procedural vs Declarative
%List vs numpy for environment and agent percepts

When deciding how the knowledge base had to be implemented, there were three options to consider: Using a procedural programming language and store logic in defined data structure in that program language, or to use a declarative programming language such as \textit{Prolog} to declare links between facts in the KB, or to use a combination of procedural (for main execution) and declarative languages (for storing logic). The procedural nature of this project needed the use of a programming language with easily implementable data structures, as such \textit{Python 3} was used as it provides flexibility for procedural processes and potential for implementing declarative principles using \textit{lists}. The use of Python also makes it possible to integrate a GUI for agent operation. The knowledge  base was implemented as a list and the code base of \citep{russell2016artificial} was used to implent the KB. As mentioned in Chapter \ref{chapter:Introduction}, we also need to manage the size of the KB as it affects the performance of inference. A Python list can easily be managed and as such is a good choice for implementation.

\subsection{Agent-Environment Boundary Rules}

Restricting the agent to move within the environment is important because the propositions the agents adds to the KB should not come from sensor measurements the do not make sense in terms of how the environment "physics" works and as such, program-defined rules based on the sensor measurements were used to restrict the agent to only perceive percepts while present on "open" blocks described in \ref{sec:environment}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Environment}
\label{sec:environment}

There were two things to consider for implementation of the environment, namely, a change in environment variables per time step as well as fast update times that does not limit the simulation in any way. There were two considerations for this: using \textit{NumPy}, also known as Numerical Python to represent the environment and agent as integer variables in an array, or using a Python list, where the same scheme is required. NumPy does not allow for multiple data types in the array, while Python lists do. NumPy was used since we do not are about data types for representing the environment, and NumPy is more scalable than Python lists. The agent, open blocks, and close blocks are arbitrarily selected integers in the array.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:experimental_method}
%Explain the experimental setup.

The experiment steps can be considered as:

\begin{enumerate}
	\item Initialize agent and environment variables.
	\item Place the agent in the environment.
	\item Let the agent roam the environment for a defined time by either controlling the agent with keyboard inputs or by letting the agent select random left,right, up or down actions.
	\item Use the model checking, proof by resolution or DPLL algorithm to let the agent map a representation of the environment as a directed graph.
	\item Measure how long inference takes for each of the 3 methods while the Knowledge Base grows in size per time step.
	\item Once the agent has met the performance standard of discovering the same number of states as the number of open blocks for the agent to roam,the simulation should end.
	\item The agent should be able to use search algorithms to find paths from one reference in the representative directed graph to another by using common search algorithms.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Metrics}
%I want to measure whether I meet the of the objects.
%This section says how and what I am going to measure.
%List what you intend to measure and how do you intend to measure it.
%You can match the metrics up to the objectives.
Relating to the objectives defined in Chapter \ref{chapter:Introduction}, the following are the metrics to reach those objectives:

\begin{itemize}
 \item Logical inference - The inference algorithm with fastest inference time will be chosen if it can perform that inference in less than 1 second.
 \item KB memory management - The KB size must be kept at some relatively constant size from one time step to the next so that inference time does not take longer than in the previous time step.
 \item Learning state definitions - The number of learnt state definition should be within 10\% of the number of open blocks that the agent can roam on.
 \item Representative knowledge graphing - If we can clearly see that the representative graph matches the way we (the designers) see the environment, this objective is met.
\end{itemize}


\section{Agent-Environment Interface}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.73]{Figures/Goal2.png}
    \caption{Example Simulation of Agent in an Environment} 
    \label{fig:agent_in_env_low_level}
\end{figure}


The environment is represented as a 2D grid world as shown in Figure \ref{fig:agent_in_env_low_level} and the agent is represented as one block (the white block) in the environment. The environment consists of open blocks and closed blocks. The agent is allowed to roam on open blocks (shown as black blocks) and not allowed to move onto closed blocks (shown as grey blocks). The different simulation environments used in this project bound the agent to ensure that the agent's next position is known and not random, which would not be the case if rules were implemented in an unbounded environment that made the agent's next position unknown if the agent moved out of bounds.
%The environment is partially observable since the agent sensor only measures the surrounding blocks of the agent and is further discussed in Subsection \ref{subsec: sensor_prop_symbols}. The environment is deterministic since the agent can move from one state to another by means of an action or series of actions. The environment is further defined to be a single-agent environment as only one agent occupies the environment in this project. The environment is static because the environment does not change as the agent moves through it, i.e., the blocks in the environment are fixed. Finally, the environment is discrete since there are a finite number of actions that the agent can do in the environment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Knowledge Based Agent}

The type of agent we use in this project is a KB Agent. This agent allows us to implement the inference algorithms discussed in Sections \ref{subsubsec: Inference_model_checking} and \ref{subsubsec: Inference_PL_theorem_proving} to query the information stored in a KB over time. The execution flow of the agent to create a representation of the environment it occupies is diagrammatically shown in Figure \ref{fig:agent_in_env_low_level}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.62]{Figures/low_level_state_gen.png}
    \caption{Agent Execution Flow} 
    \label{fig:agent_in_env_low_level}
\end{figure}

The execution starts by using the agent sensor to take measurements from the environment and stores those measurements as propositional symbols with time indices in a KB consisting of action and percepts symbols as defined in Section \ref{subsec: sensor_prop_symbols}. The reason we use time indices is so that we can differentiate between sensor measurements from one time step to the next. Next, the agent needs to know which actions and percepts it experienced, as such the agent is asked a series of questions about the measurements it acquired. Through the process of logical inference, the agent represents the true actions and percepts it measures as a positive literals (described in Section \ref{subsubsec:CNF}) and false percepts and actions as negative literals. The literals are then logically combined to form and generate state definitions as logical sentences with time indices. These state definitions as well as actions performed to move from one state to the next are stored as logical sentences in a list. Thus, every time step, we can form a state transition. We can use the state transitions to form a representation of the environment by evaluating the moves in the state transitions to build a directed graph. The directed graph removes the time indices of states and actions to generalise the environment representation to be independent of the movements the agent took to acquire the sensor measurements.


\subsection{Sensor Input and Reference Propositional Symbols}
\label{subsec: sensor_prop_symbols}

The sensor measurements the agent takes consists of whether the the immediate blocks surrounding the agent are open or close as well as which actions the agent performs from one time step to the next. The propositional symbols for the sensing if the surrounding blocks are open or closed are shown in Figure \ref{subfig:agent_in_env_low_level_percepts}, with each surrounding block corresponding to the propositional symbols for that block. The meaning of these symbols are explained in Table \ref{table:symbol_description}. The propositional symbols for actions are shown in  Figure \ref{subfig:agent_in_env_low_level_action}. Each coloured arrow in the figure corresponds to the direction the agent can move and which propositional symbol corresponds to that action. The meaning of these percepts are defined in Table \ref{table:symbol_description}. When the agent tries to execute an action and occupies the same block at the next time step, it means that the agent tried to move onto an closed block and it bumped against the wall, the propositional symbol for this action is $MB_{t}$ in Table \ref{table:symbol_description}.

\begin{figure}[H]
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.6]{Figures/Low_level_design_percepts.png}
    \caption{Reference for agent percepts proposition symbols at time \textit{t}} 
    \label{subfig:agent_in_env_low_level_percepts}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.37]{Figures/Low_level_design_actions.png}
    \caption{Reference for agent action proposition symbols at time \textit{t}} 
    \label{subfig:agent_in_env_low_level_action}
\end{subfigure}
\caption{Reference for propositional symbols used}
\label{fig:test}
\end{figure}



\begin{table}[H]
  \begin{center}
    
    \begin{tabular}{c|l}
    
      \textbf{Symbol} & \textbf{Description} \\ 
      \hline
      $L_t$ & The block to the left of the agent is open at time \textit{t}. \\ 
      $R_t$ & The block to the right of the agent is open at time \textit{t}. \\  
      $U_t$ & The block to the top of the agent is open at time \textit{t}. \\   
      ${UL}_t$ & The block to the upper left of the agent is open at time \textit{t}. \\  
      ${UR}_t$ & The block to the upper right of the agent open at time \textit{t}. \\  
      $B_t$ & The block to the bottom of the agent is open at time \textit{t}. \\  
      ${BL}_t$ & The block to the bottom left of the agent is open at time \textit{t}. \\  
      ${BR}_t$ & The block to the bottom right of the agent is open at time \textit{t}. \\    
      ${ML}_t$ & The agent moved left from the previous state to occupy a certain block at time \textit{t}.\\ 
      ${MR}_t$ & The agent moved right from the previous state to occupy a certain block at time \textit{t}.\\ 
      ${MU}_t$ & The agent moved up from the previous state to occupy a certain block at time \textit{t}.\\  
      ${MD}_t$ & The agent moved down from the previous state to occupy a certain block at time \textit{t}.\\  
      ${MB}_t$ & The agent bumped against a closed block from the previous state at time \textit{t}.
    \end{tabular}
  \end{center}
\caption{Meanings of propositional symbols used for logical sentence design.}
\end{table}
\label{table:symbol_description}



\subsection{State Sentence Formulation and Generation}

The partial states of the environment the agent can experience can be expressed as logical sentence of ANDs of the surrounding blocks the agent perceives in the state as well as the action(s) the agent performed or did not perform to get to that state. The percepts sentence components can be summarised as a column vector as seen in Equation \ref{eq:percept_vector} and the action sentence can also be summarised this way as shown in Equation \ref{eq:action_vector}. The state sentence $\mathcal{S}_t$ can then be represented as the implication of logical ANDs of the percept and action sentences at time \textit{t} as seen in Equation \ref{eq:state_vector}.

\begin{multicols}{2}
  \begin{equation}
    {P_t}=
    \begin{bmatrix}
    		(L_t \vee \neg L_t)\\
    		(R_t \vee \neg R_t)\\
    		(U_t \vee \neg U_t)\\
    		(UL_t \vee \neg UL_t)\\
    		(UR_t \vee \neg UR_t)\\
    		(B_t \vee \neg B_t)\\
    		(BL_t \vee \neg BL_t)\\
    		(BR_t \vee \neg BR_t)\\
    \end{bmatrix}
    \label{eq:percept_vector}
  \end{equation}\break
  \begin{equation}
    {A_t}=
    \begin{bmatrix}
    		(ML_t \vee \neg ML_t)\\
    		(MR_t \vee \neg MR_t)\\
    		(MU_t \vee \neg MU_t)\\
    		(MD_t \vee \neg MD_t)\\
    		(MB_t \vee \neg MB_t)\\
    \end{bmatrix}
     \label{eq:action_vector}
  \end{equation}
\end{multicols}



\begin{equation}
      \mathcal{S}_t \Rightarrow AND ({P_t}^T) \wedge AND ({A_t}^T)
 \label{eq:state_vector}
\end{equation}


State definitions generlised by Equation \ref{eq:state_vector} are generated through logical inference as the agent moves through the environment. This logical inference process is shown in Figure \ref{fig:agent_in_env_low_level}. The inference procedure uses the KB consisting of actions and percepts. This KB can be cleared after inference is carried out from time step to the next to allow for a constant KB size. Therefore, the inference process will be executed quickly to build state sentences. While the agent moves through the environment, it builds a list of logical state definitions and actions that form a state transition from one time step to the next. This list will allow us to build a directed graph that forms a representation of the environment from state transitions. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Environment Representation Creation}

The structural nature of a directed graphs, i.e., that they contain vertices and edges were suitable to represent the environment. States could be represented as vertices and directed edges between vertices could represent action(s) to transition from one state to another. Each vertex has a label and a description, the state name is stored as the label and the a state description is stored in vertex memory for retrieval after the environment representation is created. State definitions are expanded on as the agent the agent moves through the environment (and thereby the directed graph). This is possible since the percepts for the action sentence might be the same for that position in the environment, but the action sentence forming part of the state definition can be different, put simply, the agent can reach the same state in different ways and therefore the state definition for that state can be expanded over time. 
There are two possible ways for the agent to interpret a state transition. Either the agent can move to another block, or it can bump against a closed block and stay in the same position and therefore the same state:


\begin{enumerate}
	\item The agent bumps against a closed block. An edge looping to the same vertex can be created since this will signify that the agent did not move when trying to execute an action.
	\item The agent did not bump into a closed block and transitioned to another state by means of moving left, right, up or down. An edge can be created from one vertex to another representing the transition.
\end{enumerate}

These two state transition generalisations are conceptualised in Figure \ref{fig:state_diagram_concept} and further shown in Algorithm \ref{algorithm:Directed_Graph_building_algorithm}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/Low_level_design_directed_graph.png}
    \caption{Possible state transitions in terms of a directed graph} 
    \label{fig:state_diagram_concept}
\end{figure}


\vspace{0.5cm}
\begin{algorithm}[H]
\label{algorithm:Directed_Graph_building_algorithm}
\caption{\textsc{Directed Graph Building Algorithm}}
\SetAlgoLined
\DontPrintSemicolon
\KwIn{$S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$, A State Transition,  \newline $\mathcal{B} = \{\}$, An empty list to store move history}
\KwOut{$G=(V,E)$, A Directed Graph Representation of the Environment}
\textbf{Begin} \\
\Indp{
	$\mathcal{A}$ = \textsc{Extract Moves} \{$S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$\}\\
	\lIf{$\mathcal{A}$ contains $MB_t$ and ($ML_t$ or $MR_t$ or $MU_t$ or $MD_t$) }{\\
		\hspace{1cm} $S_t = S_{t+1}$ 		\\
		\hspace{1cm} Create vertex with label $S_{t}$ if not already created	\\
		\hspace{1cm} Create an edge looping from $S_t$ to $S_{t+1}$ (Same vertex) with action label\\
		\hspace{1cm} Store state definition in vertex memory\\
	}	
	
	\lIf{$\mathcal{A}$ contains only ($ML_t$ or $MR_t$ or $MU_t$ or $MD_t$)}{\\
		\hspace{1cm} Create vertex with label $S_{t}$ if not already created	\\		
		\hspace{1cm} Create vertex with label $S_{t+1}$	\\	
		\hspace{1cm} Create an edge from $S_t$ to $S_{t+1}$ with action label\\
		\hspace{1cm} Store state definition in vertex $S_{t+1}$ memory\\
		\hspace{1cm} $\mathcal{B} \leftarrow \{\mathcal{B}, \mathcal{A} \}$\\
	}	
	
	\lIf{$\mathcal{B}$ has a move sequence showing the agent is in an discovered state}{\\
		\hspace{1cm} Backtrack to the original vertex\\
		\hspace{1cm} Create an edge from $S_t$ to the backtracked vertex with action label\\
		\hspace{1cm} Store state definition in backtracked vertex memory\\
		\vspace{0.1cm}
		return Updated $G(V, E)$
	}	
	
	}
\Indm 
\textbf{End}   \\
\end{algorithm}
\vspace{0.5cm}

Algorithm \ref{algorithm:Directed_Graph_building_algorithm} is used by the agent to build the directed graph from the list of state transitions. The input to the algorithm per time step is one state transition from state transition list, $S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$. The function also keeps a constant list variable for the history of the actions the agent made, excluding actions when the agent did not move. This list is arbitrarily named $\mathcal{B}$. 

As the agent moves through the environment, it builds the directed graph $G$. There are 3 cases the agent considers which are shown as if-statements in lines 3, 9, and 16 and are explained in subsequent lines according to the meaning of the percepts in Table \ref{table:symbol_description}. 

Once the graph has been fully built by the agent using Algorithm \ref{algorithm:Directed_Graph_building_algorithm} (the number of vertices must equal the number of blocks the agent can freely roam on since each of these blocks represent a different state the agent can be in), each vertex in $G$ will contain a state description that can fully define a particular state of the agent in the environment. As stated before, each vertex is joined to another by means of edges representing actions for state state transitions. The agent must uses the state definitions to perform inference to identify where itis in the environment.
