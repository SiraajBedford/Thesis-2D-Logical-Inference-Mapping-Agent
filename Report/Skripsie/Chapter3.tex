%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ELO 3, 6
% Planning for System Design

% Chapter 3: Agent Design

% System Diagram with functional blocks 

% 3.1.1 Knowledge Base
% To store knowledge I could have used option 1,2 or 3, (List options at disposal) No indepth level. It is important you introduced the examiner to the concepts in Chapter 2 to see the requirements for such a functional block. 

% 3.1.2 Functional Block 2 ...

% *Put intefaces between functional block, e.g. I2C, SPI for an electrical project. 

% 3.2 Metrics
% I want to measure whether I meet the objectives 1,2,3. This will say how/what I will measure. What can actualy be masued and what cannot be measured. Each metrics must prove that you can reach the requirement.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=.35]{Figures/main_architecture.PNG}
%    \caption{High level mapping agent and environment design diagram} 
%    \label{fig:agent_and_environment_design}
%\end{figure}
%
%%Fill the functional blocks with more "color"
%%Also put interfaces between blocks using arrows. dsdfs
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{System Architecture}
%%Show agent in environment with robot in environment, actuators, remember agent controls the robot and the robot is in the physical environment.
%%Show all possible choices you could have made for reach of the functional blocks.
%
%This chapter goes over in detail how the agent-environment interaction is to be designed on a high level. Figure \ref{fig:agent_and_environment_design} shows the high-level design of the agent with respect to the environment.
%It is obvious that the environment is minimalistic in defined structure since it is the purpose of the agent to map the environment without knowing anything about the environment. The only narrative the agent can build of the environment is from the measurements it \textit{perceives} with its sensors. The agent is based on logic and thus it needs a logic engine, which underpins most of the operation of the agent due to the agent needing a \textit{knowledge representation language} mentioned in section \ref{sec:kb_agents} to convey knowledge.
%It must also be conveyed that the agent in Figure \ref{fig:agent_and_environment_design} does not necessarily mirror the general learning agent in section \ref{sec:agents_and_environments}. This is because the project captured in this report builds on the concepts describing agents based on knowledge bases in section \ref{sec:kb_agents}. The purpose of section \ref{sec:agents_and_environments} was to introduce the reader to the concept of \textit{an agent in an environment}. The mapping agent 
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% reference objectives: Logical inference, KB memory management, learning state definitions, representative knowledge graphing 
%
%\section{Mapping Agent}		
%
%\subsection{Logic Engine}
%The logic engine is what allows the agent's internal processes to communicate. Thus, it had to have the the ability to handle two processes quickly and efficiently, namely \textit{sentence management} as well as the ability to \textit{perform logic operations and equivalences} on those sentences. There were two options to implement a logic engine: either write one from scratch, or use an existing logic engine with known performance, i.e., if it is suitable for the context of this project. Since the aim of this project is not to build a logic engine, source code written with an MIT license was used. This code base was based on direct examples written in \citep{russell2016artificial} and as such, was highly suitable and implementable.
%
%\subsection{Inference Engine}
%The inferential component of the system is the core component we need to design as it will allow the agent to come to conclusions about the environment as the agent moves through the environment. There are three algorithms that were considered when formulating an inference plan, namely, model checking, proof by resolution and the use of the DPLL algorithm to prove unsatisfiability. All three of these inference methods are detailed in section \ref{sec:logical_inference}. All three algorithms will be experimented with as described in section \ref{sec:experimental_method}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=.6]{Figures/Inference_HL.PNG}
%    \caption{Inference Engine} 
%    \label{fig:inference_engine}
%\end{figure}
%
%\subsection{Knowledge Base}
%%Procedural vs Declarative
%%List vs numpy for environment and agent percepts
%
%When deciding how the knowledge base had to be implemented, there were three options to consider: Using a procedural programming language and store logic in defined data structure in that program language, or to use a declarative programming language such as \textit{Prolog} to declare links between facts in the KB, or to use a combination of procedural (for main execution) and declarative languages (for storing logic). The procedural nature of this project needed the use of a programming language with easily implementable data structures, as such \textit{Python 3} was used as it provides flexibility for procedural processes and potential for implementing declarative principles using \textit{lists}. The use of Python also makes it possible to integrate a GUI for agent operation. The knowledge  base was implemented as a list and the code base of \citep{russell2016artificial} was used to implent the KB. As mentioned in Chapter \ref{chapter:Introduction}, we also need to manage the size of the KB as it affects the performance of inference. A Python list can easily be managed and as such is a good choice for implementation.
%
%\subsection{Agent-Environment Boundary Rules}
%
%Restricting the agent to move within the environment is important because the propositions the agents adds to the KB should not come from sensor measurements the do not make sense in terms of how the environment "physics" works and as such, program-defined rules based on the sensor measurements were used to restrict the agent to only perceive percepts while present on "open" blocks described in \ref{sec:environment}.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Environment}
%\label{sec:environment}
%
%There were two things to consider for implementation of the environment, namely, a change in environment variables per time step as well as fast update times that does not limit the simulation in any way. There were two considerations for this: using \textit{NumPy}, also known as Numerical Python to represent the environment and agent as integer variables in an array, or using a Python list, where the same scheme is required. NumPy does not allow for multiple data types in the array, while Python lists do. NumPy was used since we do not are about data types for representing the environment, and NumPy is more scalable than Python lists. The agent, open blocks, and close blocks are arbitrarily selected integers in the array.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Methodology}
%\label{sec:experimental_method}
%%Explain the experimental setup.
%
%The experiment steps can be considered as:
%
%\begin{enumerate}
%	\item Initialize agent and environment variables.
%	\item Place the agent in the environment.
%	\item Let the agent roam the environment for a defined time by either controlling the agent with keyboard inputs or by letting the agent select random left,right, up or down actions.
%	\item Use the model checking, proof by resolution or DPLL algorithm to let the agent map a representation of the environment as a directed graph.
%	\item Measure how long inference takes for each of the 3 methods while the Knowledge Base grows in size per time step.
%	\item Once the agent has met the performance standard of discovering the same number of states as the number of open blocks for the agent to roam,the simulation should end.
%	\item The agent should be able to use search algorithms to find paths from one reference in the representative directed graph to another by using common search algorithms.
%\end{enumerate}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Agent Design} 
\label{chapter:Agent_Design}

The design of the agent to meet the objectives of the project in Chapter \ref{chapter:Introduction} is made in this chapter. The design of the agent in this chapter addresses the high-level and low-level aspects for an agent that can map its environment using a logic-based approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Agent-Environment Interface}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.73]{Figures/example_sim_chap3}
    \caption{Example Simulation of Agent in an Environment} 
    \label{fig:agent_in_env_low_level}
\end{figure}

The aim of this project is to place an agent in an environment and then let that agent discover the structure of the environment for itself. Such an environment can be represented as a 2D grid world as shown in Figure \ref{fig:agent_in_env_low_level} and the agent is represented as one block (the yellow block) in the environment. The environment consists of open blocks and closed blocks. The agent is allowed to roam on open blocks (shown as black blocks) and not allowed to move onto closed blocks (shown as red blocks). The environment in Figure \ref{fig:agent_in_env_low_level} restricts the agent's movement to the black blocks because the borders of the environment consists of closed blocks. This is the case for all environments used in this project for simulation. As the agent moves through the environment it uses sensors to gather information. The modeling of these sensors are discussed in Section \ref{subsec: sensor_prop_symbols}
%The environment is partially observable since the agent sensor only measures the surrounding blocks of the agent and is further discussed in Subsection \ref{subsec: sensor_prop_symbols}. The environment is deterministic since the agent can move from one state to another by means of an action or series of actions. The environment is further defined to be a single-agent environment as only one agent occupies the environment in this project. The environment is static because the environment does not change as the agent moves through it, i.e., the blocks in the environment are fixed. Finally, the environment is discrete since there are a finite number of actions that the agent can do in the environment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Sensor Input and Reference Propositional Symbols}
\label{subsec: sensor_prop_symbols}

For the agent to map the structure on it's environment, it has to be able to distinguish between different states of the environment, where the different states are all the blocks the agent can occupy. This can be done using sensors that model what the agent can perceives as propositional symbols. The sensor measurements the agent takes consists of whether the the immediate blocks surrounding the agent are open or close as well as which actions the agent performs from one time step to the next. The propositional symbols for the sensing if the surrounding blocks are open or closed are shown in Figure \ref{subfig:sensor_percepts}, with each surrounding block corresponding to the propositional symbols for that block. The meaning of these symbols are explained in Table \ref{table:symbol_description}. The propositional symbols for actions are shown in  Figure \ref{subfig:sensor_actions}. Each coloured arrow in the figure corresponds to the direction the agent can move and which propositional symbol corresponds to that action. The meaning of these action propositional symbols are defined in Table \ref{table:symbol_description}.

\begin{figure}[H]
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.6]{Figures/Low_level_design_percepts.png}
    \caption{Reference for agent percepts proposition symbols at time \textit{t}} 
    \label{subfig:sensor_percepts}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.37]{Figures/Low_level_design_actions.png}
    \caption{Reference for agent action proposition symbols at time \textit{t}} 
    \label{subfig:sensor_actions}
\end{subfigure}
\caption{Reference for propositional symbols used}
\label{fig:test}
\end{figure}



\begin{table}[H]
  \begin{center}
    
    \begin{tabular}{c|l}
    
      \textbf{Symbol} & \textbf{Description} \\ 
      \hline
      $L_t$ & The block to the left of the agent is open at time \textit{t}. \\ 
      $R_t$ & The block to the right of the agent is open at time \textit{t}. \\  
      $U_t$ & The block to the top of the agent is open at time \textit{t}. \\   
      ${UL}_t$ & The block to the upper left of the agent is open at time \textit{t}. \\  
      ${UR}_t$ & The block to the upper right of the agent open at time \textit{t}. \\  
      $B_t$ & The block to the bottom of the agent is open at time \textit{t}. \\  
      ${BL}_t$ & The block to the bottom left of the agent is open at time \textit{t}. \\  
      ${BR}_t$ & The block to the bottom right of the agent is open at time \textit{t}. \\    
      ${ML}_t$ & The agent moved left from the previous state to occupy a certain block at time \textit{t}.\\ 
      ${MR}_t$ & The agent moved right from the previous state to occupy a certain block at time \textit{t}.\\ 
      ${MU}_t$ & The agent moved up from the previous state to occupy a certain block at time \textit{t}.\\  
      ${MD}_t$ & The agent moved down from the previous state to occupy a certain block at time \textit{t}.\\  
    \end{tabular}
  \end{center}
\caption{Meanings of propositional symbols used for logical sentence design.}
\label{table:symbol_description}

\end{table}

The agent \textbf{does not} have access to symbol descriptions (meaning of symbol) of the symbols in Table \ref{table:symbol_description}. The purpose of mentioning the descriptions of the symbols in Table \ref{table:symbol_description} is so that a human can interpret the meaning of the logical sentences they can combine to form. If \textbf{any} of the percepts propositional symbols have a negation operator attached, then that block is closed and the agent cannot roam on it.


\subsection{State Definitions and State Transitions}
\label{subsec:state_sentence_form}

The partial states of the environment the agent can experience can be expressed as logical sentence of conjunctions of the surrounding blocks the agent perceives as logcial symbols as in Figure \ref{subfig:sensor_percepts}. These partial states are referred to as ``state definitions''. An example of a state definition containing only proposition symbols representing the surrounding blocks as open blocks is shown in Equation \ref{eq:state_def_ex}:

\begin{equation}
 	\mathcal{S}_n \Rightarrow L_t \wedge R_t \wedge U_t \wedge UL_t \wedge UR_t \wedge B_t \wedge BL_t \wedge BR_t.
 	\label{eq:state_def_ex}
\end{equation}

The blocks surrounding an agent may not always be open. As such, another example logical sentence for the state definition the agent can acquire in Figure \ref{fig:agent_in_env_low_level} is shown in Equation \ref{eq:state_def_ex2}:

\begin{equation}
 	\mathcal{S}_m \Rightarrow L_t \wedge \neg R_t \wedge U_t \wedge UL_t \wedge \neg UR_t \wedge \neg B_t \wedge \neg BL_t \wedge \neg BR_t.
 	\label{eq:state_def_ex2}
\end{equation}

State definitions can be combined with actions to form state transitions. This is shown in Equation \ref{eq:state_trans}, where $\mathcal{A}$ represents an action the agent can take. The variable $t$ indicates times and shows that when doing an action from state $\mathcal{S}_t$ we can end up at different state or the same state (dependent on the action and state definition of $\mathcal{S}_t$) at the next time step, $\mathcal{S}_{t+1}$:



\begin{equation}
 	\mathcal{S}_t \wedge \mathcal{A} \Rightarrow  \mathcal{S}_{t+1}.
 	\label{eq:state_trans}
\end{equation}

%\begin{equation}
%      \mathcal{S}_t \Rightarrow AND ({P_t}^T) \wedge AND ({A_t}^T)
% \label{eq:state_vector}
%\end{equation}


State transitions can be used to represent the structure of the map since if we have all state trasitions inof an environment, we can link all the states together using actions -- this would form a representation of an environment. 

%State definitions can  generated through logical inference as the agent moves through the environment. This logical inference process is shown in Figure \ref{fig:agent_in_env_low_level}. The inference procedure uses the KB consisting of actions and percepts. This KB can be cleared after inference is carried out from time step to the next to allow for a constant KB size. Therefore, the inference process will be executed quickly to build state sentences. While the agent moves through the environment, it builds a list of logical state definitions and actions that form a state transition from one time step to the next. This list will allow us to build a directed graph that forms a representation of the environment from state transitions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Knowledge Based Agent}
\label{sec:kb_agentz}

The type of agent we use in this project is a KB Agent. This agent allows us to implement the inference algorithms discussed in Sections \ref{subsec:Inference_DPLL}, \ref{subsec:Inference_Model_Checking} and \ref{subsec:Inference_Resolution} to query the information stored in a knowledge base (KB) over time. The design of the KB agent used in this project is shown in the Figure \ref{fig:agent_design}.
The execution flow and design motivation (for the main components of the agent) for the agent with reference to Figure \ref{fig:agent_design}is as follows:

\begin{enumerate}
\item The Inference Engine is initialised for the use of logical inference using one of the inference algorithms forumlated in Sections \ref{subsec:Inference_DPLL}, \ref{subsec:Inference_Resolution} or \ref{subsec:Inference_Model_Checking}.
 \item The agent begins by gathering many state definitions from the environment using Sensor 1, even if the state definitions are similar. These state definitions are then stored in ``\textbf{KB A}''. The purpose of this KB is to store all the state definitions the agent first acquires.
 \item After the agent has acquired enough state definitions in ``\textbf{KB A}'', the agent again moves through the environment and infers which state it is in using the state definitions stored in ``\textbf{KB A}''. A separate KB, ```\textbf{KB B}'', is used to store only states that are unique that the agent experiences while roaming the environment. The purpose of storing the unique state definitions is to manage the KB size so that the agent can perform quicker inference when learning the structure of the environment. Sensor 2 receives the agent's current percepts and converts the percepts to a logical conjunction of percepts P1. 
``\textbf{KB B}'' is first checked to see if contains P1, if it is, it is removed from ``\textbf{KB A}'' to reduced ``\textbf{KB B}'' in size to perform inference quicker. If P1 is not contained in ``\textbf{KB B}'' , ``KB A'' is checked for P1 by inference. if ``KB A'' contains P1, the unique state definition is added to ``\textbf{KB B}'' . If ``\textbf{KB A}'' does not contain P1, the agent is stopped and the simulation must be reran from the start because the agent did not learn P1 from ``\textbf{KB A}''.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{Figures/low_level_state_gen.png}
    \caption{Knowledge-based agent design.} 
    \label{fig:agent_design}
\end{figure}

\item After the agent has learned all unique state definitions from the environment and they are all contained in ``\textbf{KB B}'', the agent again roams the environment to form state transitions. Sensor 3 receives the agent's current percepts and converts the percepts to a logical conjunction of percepts P2. Each unique state in ``\textbf{KB B}'' has a unique state name. P2 is checked against the states in ``\textbf{KB B}'', and the state name of the corresponding state definition of P2 is returned. The inferred state is the current state of the agent. The action the agent did to get to it's current state is then inferred by using ``\textbf{KB C}'' and asking if the agent moved left,right, up, or down. Using the current state and inferred action, a state transition sentence can be stored in ``\textbf{KB D}''.
\item The previous state becomes the current state when the process is repeated again. This process is repeated until the agent has all required state transitions to represent the structure of its environment. 
\item Once all state transitions are learnt by the agent, the agent converts the representation into a KB representation so that agent can make predictions about future states it can be in.

\end{enumerate}

The agent designed in Figure \ref{fig:agent_design} has to be implemented and tested. The implementation of the agent is discussed in Chapter \ref{chapter: implementation_and_setup}. The experimental setup to obtain results in Chapter \ref{chapter:Results} is also explained in Chapter \ref{chapter: implementation_and_setup}.



%The execution starts by using the agent sensor to take measurements from the environment and stores those measurements as propositional symbols with time indices in a KB consisting of action and percepts symbols as defined in Section \ref{subsec: sensor_prop_symbols}. The reason we use time indices is so that we can differentiate between sensor measurements from one time step to the next. Next, the agent needs to know which actions and percepts it experienced, as such the agent is asked a series of questions about the measurements it acquired. Through the process of logical inference, the agent represents the true actions and percepts it measures as a positive literals (described in Section \ref{subsubsec:CNF}) and false percepts and actions as negative literals. The literals are then logically combined to form and generate state definitions as logical sentences with time indices. These state definitions as well as actions performed to move from one state to the next are stored as logical sentences in a list. Thus, every time step, we can form a state transition. We can use the state transitions to form a representation of the environment by evaluating the moves in the state transitions to build a directed graph. The directed graph removes the time indices of states and actions to generalise the environment representation to be independent of the movements the agent took to acquire the sensor measurements.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\section{Environment Representation Creation}
%
%The structural nature of a directed graphs, i.e., that they contain vertices and edges were suitable to represent the environment. States could be represented as vertices and directed edges between vertices could represent action(s) to transition from one state to another. Each vertex has a label and a description, the state name is stored as the label and the a state description is stored in vertex memory for retrieval after the environment representation is created. State definitions are expanded on as the agent the agent moves through the environment (and thereby the directed graph). This is possible since the percepts for the action sentence might be the same for that position in the environment, but the action sentence forming part of the state definition can be different, put simply, the agent can reach the same state in different ways and therefore the state definition for that state can be expanded over time. 
%There are two possible ways for the agent to interpret a state transition. Either the agent can move to another block, or it can bump against a closed block and stay in the same position and therefore the same state:
%
%
%\begin{enumerate}
%	\item The agent bumps against a closed block. An edge looping to the same vertex can be created since this will signify that the agent did not move when trying to execute an action.
%	\item The agent did not bump into a closed block and transitioned to another state by means of moving left, right, up or down. An edge can be created from one vertex to another representing the transition.
%\end{enumerate}
%
%These two state transition generalisations are conceptualised in Figure \ref{fig:state_diagram_concept} and further shown in Algorithm \ref{algorithm:Directed_Graph_building_algorithm}.
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.6]{Figures/Low_level_design_directed_graph.png}
%    \caption{Possible state transitions in terms of a directed graph} 
%    \label{fig:state_diagram_concept}
%\end{figure}
%
%
%\vspace{0.5cm}
%\begin{algorithm}[H]
%\label{algorithm:Directed_Graph_building_algorithm}
%\caption{\textsc{Directed Graph Building Algorithm}}
%\SetAlgoLined
%\DontPrintSemicolon
%\KwIn{$S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$, A State Transition,  \newline $\mathcal{B} = \{\}$, An empty list to store move history}
%\KwOut{$G=(V,E)$, A Directed Graph Representation of the Environment}
%\textbf{Begin} \\
%\Indp{
%	$\mathcal{A}$ = \textsc{Extract Moves} \{$S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$\}\\
%	\lIf{$\mathcal{A}$ contains $MB_t$ and ($ML_t$ or $MR_t$ or $MU_t$ or $MD_t$) }{\\
%		\hspace{1cm} $S_t = S_{t+1}$ 		\\
%		\hspace{1cm} Create vertex with label $S_{t}$ if not already created	\\
%		\hspace{1cm} Create an edge looping from $S_t$ to $S_{t+1}$ (Same vertex) with action label\\
%		\hspace{1cm} Store state definition in vertex memory\\
%	}	
%	
%	\lIf{$\mathcal{A}$ contains only ($ML_t$ or $MR_t$ or $MU_t$ or $MD_t$)}{\\
%		\hspace{1cm} Create vertex with label $S_{t}$ if not already created	\\		
%		\hspace{1cm} Create vertex with label $S_{t+1}$	\\	
%		\hspace{1cm} Create an edge from $S_t$ to $S_{t+1}$ with action label\\
%		\hspace{1cm} Store state definition in vertex $S_{t+1}$ memory\\
%		\hspace{1cm} $\mathcal{B} \leftarrow \{\mathcal{B}, \mathcal{A} \}$\\
%	}	
%	
%	\lIf{$\mathcal{B}$ has a move sequence showing the agent is in an discovered state}{\\
%		\hspace{1cm} Backtrack to the original vertex\\
%		\hspace{1cm} Create an edge from $S_t$ to the backtracked vertex with action label\\
%		\hspace{1cm} Store state definition in backtracked vertex memory\\
%		\vspace{0.1cm}
%		return Updated $G(V, E)$
%	}	
%	
%	}
%\Indm 
%\textbf{End}   \\
%\end{algorithm}
%\vspace{0.5cm}
%
%Algorithm \ref{algorithm:Directed_Graph_building_algorithm} is used by the agent to build the directed graph from the list of state transitions. The input to the algorithm per time step is one state transition from state transition list, $S_t \xrightarrow[\text{}]{\mathcal{A}} S_{t+1}$. The function also keeps a constant list variable for the history of the actions the agent made, excluding actions when the agent did not move. This list is arbitrarily named $\mathcal{B}$. 
%
%As the agent moves through the environment, it builds the directed graph $G$. There are 3 cases the agent considers which are shown as if-statements in lines 3, 9, and 16 and are explained in subsequent lines according to the meaning of the percepts in Table \ref{table:symbol_description}. 
%
%Once the graph has been fully built by the agent using Algorithm \ref{algorithm:Directed_Graph_building_algorithm} (the number of vertices must equal the number of blocks the agent can freely roam on since each of these blocks represent a different state the agent can be in), each vertex in $G$ will contain a state description that can fully define a particular state of the agent in the environment. As stated before, each vertex is joined to another by means of edges representing actions for state state transitions. The agent must uses the state definitions to perform inference to identify where itis in the environment.
